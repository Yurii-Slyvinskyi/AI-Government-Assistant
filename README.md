<div align="center">

# **ğŸ¤– AI Government Assistant**

**AI Government Assistant** is an intelligent helper designed to make it easier for people to find information on
government websites.  
Built with **FastAPI, OpenAI API, Qdrant**, and other modern tools.

[ğŸ’» **Source Code**](https://github.com/Yurii-Slyvinskyi/AI-Government-Assistant)


</div>

## ğŸš€ Features

- Semantic search across website content using embeddings
- Store and index data with **Qdrant** vector database
- User-friendly **REST API** for integration with other services
- Generate human-like answers using **LLM (OpenAI API)**
- Extensible architecture (plugins, additional data sources)
- Multilingual support

## ğŸ“ Short Feature Presentation

As an example, this project uses data from the **Government of Alberta** website (https://www.alberta.ca/) â€”
specifically, the section on  
[**Non-profit and Charitable Organizations**](https://www.alberta.ca/non-profit-and-charitable-organizations-topic).

The AI Assistant is designed to:

- Help users quickly find accurate answers to their questions
- Guide users to the correct section of a website more efficiently
- Provide responses in a natural, human-like manner
- Politely redirect when questions are unrelated to the government (e.g., â€œWhat is Audi?â€) by:
    - Explaining that the assistant only provides Alberta governmentâ€“related information
    - Suggesting a related topic (e.g., car registration in Alberta)

## ğŸ“¸ Examples (Screenshots)

### Example 1 â€” Definition & Sources

User asks: *â€œWhat is a charitable organization?â€* and *â€œWhat are the main services offered on this website?â€*

- Assistant provides a **brief definition**
- Lists **key points**
- Includes **sources with links**
- **Shows** that it searches indexed content

![Charity definition](assets/charity-definition.png)

### Example 2 â€” Deeper Reasoning

User asks: *â€œWhat is the difference between a foundation and a charity?â€*

- **Assistant** shows that he has deeper thinking, not just copy-paste

![Deeper Reasoning](assets/deeper-reasoning.png)

### Example 3 â€” Off-topic Question

User asks: *â€œBMWâ€* and *â€œnike organizationsâ€*

- Assistant explains that it can only help with Alberta governmentâ€“related topics
- Redirects user to a relevant area (e.g., how to register a car in Alberta)

![Off-topic Question](assets/off-topic-question.png)

### Example 4 â€” Multilingual Support

User asks in Ukrainian: *â€œĞ©Ğ¾ Ñ‚Ğ°ĞºĞµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ñ–Ğ¹Ğ½Ğ° Ğ¾Ñ€Ğ³Ğ°Ğ½Ñ–Ğ·Ğ°Ñ†Ñ–Ñ?â€*

- Assistant responds correctly in the same language

![Multilingual Support](assets/multilingual-support.png)

## ğŸ›  Tech Stack

- [FastAPI](https://fastapi.tiangolo.com/) â€” REST API backend
- [Qdrant](https://qdrant.tech/) â€” vector database
- [OpenAI API](https://platform.openai.com/) â€” embeddings & answer generation
- [Celery](https://docs.celeryq.dev/) + [Redis](https://redis.io/) â€” async background tasks
- [Docker](https://www.docker.com/) â€” containerization & deployment
- [React](https://react.dev/) â€” frontend interface
- [Pytest](https://docs.pytest.org/) â€” testing framework

## âš™ï¸ Pipeline

### User Queries

`ai_assistant_service`  
Processes user questions and returning AI-generated answers using OpenAI.

â†’  
`embedding_service`  
Generates embedding vectors for text using OpenAI.

â†’  
`qdrant_service`  
Stores vectors (embeddings) in the Qdrant database and performs semantic vector searches.

### Data Ingestion

`ingestor_service`  
Fetches, cleans, and splits website content into smaller chunks for vector embedding.

â†’  
`embedding_service`  
Generates embedding vectors for text using OpenAI.

â†’  
`qdrant_service`  
Stores in Qdrant for future semantic search and retrieval.

## ğŸ”§ Optimization

### ğŸ§  Context Memory with `Redis`

### Current Workflow

Right now, the chatbot **does not remember past messages** â€“ every user question is treated as independent.

### 1ï¸âƒ£ Short-Term Memory with Redis

We can add short-term chat memory using **Redis** as a fast in-memory store.

- ğŸ—‚ Save the **last N messages** per user (e.g., last 10â€“20 messages).
- â³ Use a **TTL (time-to-live)**, e.g., keep history for **30 minutes after last activity**.
- âš¡ When the user asks a new question â†’ fetch their previous context from Redis â†’ append to the prompt â†’ send to LLM.

### ğŸ” Recommended Improvement Strategy

- âœ… Keep memory **short-term only** (cheap, fast).
- âš¡ Use **Redis TTL** to auto-clean inactive sessions.
- ğŸ‘¥ Handle **multi-user context** with `chat:{user_id}` keys.
- ğŸ“ˆ Scales well even with many users, since Redis is optimized for fast reads/writes.

### ğŸ“ Ingestor Service `ingestor_service`

### Current Workflow

`IngestorService` collects pages from [Alberta.ca](http://alberta.ca/), cleans the content, splits it into semantic
chunks, and asynchronously sends them via **Celery** to another service for creating embeddings. Currently, URLs are
added manually.

### 1ï¸âƒ£ Automatic URL Collection

Currently, URLs are provided manually, but this can be optimized with a Crawler service that:

- ğŸŒ If the site has a `sitemap.xml` â€“ parse it and save all URLs in the database.
- ğŸ” If there is no sitemap â€“ use a crawler that recursively navigates the main page and adds internal links to the DB.

### 2ï¸âƒ£ Automatic Content Updates

Right now, the service needs to be manually triggered when content changes. This can be optimized in two ways:

**a) Periodic run (Scheduler / Celery Beat):**  
Regularly check the site (e.g., weekly). If there are changes â†’ trigger `IngestorService`.

**b) Event-driven approach:**  
React to events (new or updated pages).  
Event â†’ `IngestorService` â†’ content cleaning â†’ chunking â†’ update embeddings in the database.

**Examples of event sources:**

- ğŸ“¡ Webhook from the site (API signals updates)
- âš¡ Signals (in CMS/Django when a page is published)
- ğŸ“‚ File Watcher / Queue (if data is in files/documents)

### ğŸ” Recommended Improvement Strategy

Combine both approaches:

- ğŸ”„ **Periodic scan** (to ensure nothing is missed)
- âš¡ **Event-driven updates** (so changes are processed immediately)
- ğŸ“š **Add more sources**, not limited to Alberta.ca pages only (PDFs, official guides, FAQs)

### ğŸ’  Embedding Service `embedding_service`

### Current Workflow

`EmbeddingService` receives text chunks from `IngestorService` and creates embedding vectors using OpenAI, then stores
them in Qdrant for semantic search. When a user asks a question, the service generates a query embedding and searches
for the closest vectors in the database.

### ğŸ”§ Possible Improvements

- âš¡ **Batch processing** â€“ processing multiple chunks at once reduces the number of requests to OpenAI and speeds up
  embedding generation.
- ğŸ§  **Embedding model** â€“ for higher accuracy, use `text-embedding-3-large` instead of `3-small`.

### ğŸ—„ï¸ Qdrant Service `qdrant_service`

### Current Workflow

- `EmbeddingService` receives text chunks from `IngestorService` and creates embeddings via OpenAI, then stores them in
  Qdrant for semantic search (RAG).
- When a user asks a question, the service generates a query embedding and finds the closest vectors in the database.

### ğŸ”§ Possible Improvements

- âš¡ **Batch processing** â€“ processing multiple chunks simultaneously speeds up operations.
- ğŸ§  **Embedding model** â€“ for higher accuracy, use `text-embedding-3-large` instead of `3-small`.
- ğŸ”„ **Alternative vector databases** â€“ for higher load or specific cases, consider alternatives (Pinecone, Milvus,
  Weaviate), although Qdrant handles production loads well.

### ğŸ¤– AI Assistant Service `ai_assistant_service`

### Current Workflow

This service receives user questions, forwards them to the `embedding_service` for processing, retrieves relevant
context, and generates answers using OpenAI.

Answers ensure:

- ğŸ“Œ based only on the provided context;
- ğŸŒ support multilingual output;
- ğŸ“ structured (definition, key points, sources, invitation to ask for clarification).

### ğŸ”§ Improvements

**LLM Model**

- âš¡ `gpt-4o-mini` is fast and cheap, but answers may be dry.
- ğŸ§  `gpt-4o` or `gpt-4o-large` â†’ improves human-likeness and answer accuracy.
- ğŸ”€ Can combine: fast mini for simple questions, large model for complex or ambiguous queries.

**Prompt / Instructions**

- âœï¸ Clearly define tone and style (friendly, expert, consultative).
- ğŸ§‘â€ğŸ’¼ Add roles and context instructions (e.g., â€œYou are a consultant for Alberta government servicesâ€¦â€).
- ğŸ” Add checks for off-topic or incomplete data, so the model always provides useful information.

**Optimization**

- ğŸ’¾ Cache embeddings for popular queries to avoid repeated OpenAI requests.
- â±ï¸ Limit tokens in the prompt for faster rendering.

## âš¡ Quickstart

### ğŸ“‹ Prerequisites

Before running the project, make sure you have the following installed:

- ğŸ³ [Docker Desktop](https://www.docker.com/products/docker-desktop)
- ğŸ“¦ Docker Compose (comes with Docker Desktop)
- ğŸ§¬ Git
- ğŸ” `.env` file with all required environment variables  
  _(see `.env.example` in the repository for reference)_

### ğŸ³ Install Docker

1. Download and install [Docker Desktop](https://www.docker.com/products/docker-desktop) for your OS (Windows, macOS, or
   Linux)
2. **Start Docker Desktop manually**
3. Verify that Docker is running by running the **following command**:

```
docker info
```

### âœ… Verify Docker Installation

If you see output with information about `Server`, `Containers`, `Images` â€” you're good to go âœ…

If you get an **error** like:

```
Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
```

ğŸ”§ Make sure Docker is **running**
(on macOS â€” open **Docker Desktop** and wait until it fully starts)

### ğŸš€ Steps to Run the Project

#### 1ï¸âƒ£ Clone the Repository

```
git clone https://github.com/Yurii-Slyvinskyi/AI-Government-Assistant.git
cd AI-Government-Assistant
```

#### 2ï¸âƒ£ Create `.env`

Copy the examples files `.env.example` in every backend service and frontend:

```
cp .env.example .env
```

**Fill in all** the environment variables based on **your own keys**

#### 3ï¸âƒ£ Start Docker Containers for Development

```
docker compose up --build
```

â³ The first launch may take a few minutes â€” Docker images will be built, migrations will run, and services will
initialize

#### 4ï¸âƒ£ Done!

The API will be available at ğŸ‘‰ http://localhost:3000/

#### ğŸ’¡ If you get errors or Docker doesnâ€™t start â€” make sure Docker Desktop is running and fully loaded

## ğŸ§ª Testing

To run tests, use:

### ai_assistant_service:

```
docker compose -f docker-compose-test.yml run --rm ai_assistant_tests
```

### embedding_service:

```
docker compose -f docker-compose-test.yml run --rm embedding_service_tests
```

### ingestor_service:

```
docker compose -f docker-compose-test.yml run --rm ingestor_service_tests
```

### qdrant_service:

```
docker compose -f docker-compose-test.yml run --rm qdrant_service_tests
```

## ğŸ“¬ Contact me

#### ğŸ“ +1 (780) 224 7457

#### ğŸ“§ [yura.programing@gmail.com](mailto:yura.programing@gmail.com)

#### ğŸ’¼ [LinkedIn](https://www.linkedin.com/in/yurii-slyvinskyi-827831284)

#### ğŸ“· [Instagram](https://www.instagram.com/yura.prg)

#### ğŸ’¬ [Telegram ](https://t.me/yurassslyv)